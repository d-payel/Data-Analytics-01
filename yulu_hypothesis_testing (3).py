# -*- coding: utf-8 -*-
"""YULU Hypothesis Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QDfDup-8DaWlHHJfJrvJEyG6shuOFI6Z
"""

!gdown "https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/001/428/original/bike_sharing.csv?1642089089" -O yulu.csv

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import scipy
from scipy.stats import norm
from sklearn.preprocessing import StandardScaler
from scipy import stats
import statsmodels.api as sm
from scipy.stats import shapiro
import warnings
warnings.filterwarnings('ignore')

import scipy.stats as stats

df=pd.read_csv('yulu.csv')
df.head()

df.isnull().any()

df.isna().any()

df.describe()

palette = sns.cubehelix_palette(9, start=5, rot=.6)
colors=palette.as_hex()

df['datetime'] = pd.to_datetime(df['datetime'])
df['date'] = df['datetime'].dt.date
df['time'] = df['datetime'].dt.time
df.head()

df['holiday'].unique()

# Mapping The Seasons and Weekdays and Holidays
season_mapping={1:'Spring',2:'Summer',3:'Fall',4:'Winter'}
df['season']=df['season'].map(season_mapping)
holiday_mapping={0:'No', 1:'Holiday'}
workingday_mapping={0:'No', 1:'Workingday'}
df['holiday']=df['holiday'].map(holiday_mapping)
df['workingday']=df['workingday'].map(workingday_mapping)
df['day_type']=df['holiday']+df['workingday']
df.head()

df['day_type'].unique()

map={'NoNo':'weekend','NoWorkingday':'weekday','HolidayNo':'holiday'}
df['day_type']=df['day_type'].map(map)
df.head()

df['weather']=df['weather'].astype('category')

df['weather'].dtype

[print(f'{df[i].dtype}') for i in ['season','weather','date','time']]

df['atemp_windspeed']=df['atemp']*df['windspeed']

plt.figure(figsize=(10,10))
sns.set_palette(colors[3:6])
sns.pairplot(data=df)
plt.show()

"""Both the description of Numerical Columns and pairplots depict that
Temp, Atemp, Windspeed, Casual, Registered and Count all of these columns are likely to have outliers
"""

df_copy=df.copy()

df_copy.head(2)

df_copy['date']=pd.to_datetime(df_copy['date'])
df_copy['YYYY-MM']=df_copy['date'].dt.strftime('%Y-%m')
month_year_df=df_copy.groupby('YYYY-MM')['count'].sum().reset_index()
month_year_df.head()

time_df=pd.DataFrame(df.groupby('time')['count'].sum())
time_df.reset_index(inplace=True)
time_df.head(2)

mode_hours=time_df['count'].nlargest(3).index
plt.figure(figsize=(14,5))
sns.set_palette(colors[3:6])
sns.lineplot(x=time_df.index,y=time_df['count'])
plt.xlabel('Hours')
plt.xticks(np.arange(0,25,1))
for mode_hour in mode_hours:
  plt.axvline(x=mode_hour, color='gray', linestyle='--')
plt.show()

"""**Rental activity peaks at 8 am and 5 and 6 pm**"""

plt.figure(figsize=(14,5))
sns.set_palette(colors[3:6])
sns.lineplot(data=month_year_df,x='YYYY-MM',y='count')
plt.xlabel('Year-Month')
plt.xticks(rotation=45)
plt.show()

"""**The graph illustrates a seasonal trend in rental counts, which declines each January and peaks in during middle of the year. The dip in January 2011 is more severe than the dip in January 2012, while the peak in June 2011 is lower than the peak in June 2012. This indicates a worsening trend at the start of 2011, followed by a recovery in rental activity by mid-2012.**

"""

df_copy['date'] = pd.to_datetime(df_copy['date'])
df_copy['month'] = df_copy['date'].dt.month
df_copy['year'] = df_copy['date'].dt.year
df_copy['day'] = df_copy['date'].dt.day
df_copy.drop(['date','YYYY-MM'],axis=1,inplace=True)
month_map={1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'Jun',7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'}
df_copy['day']=pd.Categorical(df_copy['day'])
df_copy['year']=pd.Categorical(df_copy['year'])
df_copy['month']=df_copy['month'].map(month_map)
df_copy.head()

df_copy.groupby('year')['casual'].sum()

df_copy.groupby('year')['registered'].sum()

"""**From 2011 to 2012, the overall number of bike rentals has increased as well as number of Registered Customers increased.**"""

plt.figure(figsize=(7,7))
sns.set_palette(colors[1:8:3])
sns.barplot(x=df['weather'],y=df['count'],hue=df['day_type'])
plt.show()

"""1:Clear, Few clouds, partly cloudy, partly cloudy<br>
2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist<br>
3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds<br>
4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog

"""

df_4=df[df['weather']==4]
df_4

"""**158 Registered and 6 Non-registered users have rented bikes even in heavy rain, ice-pallets, thunderstorm and mist only on a weekday supposedly for office purpose.**"""

plt.figure(figsize=(7,7))
sns.set_palette(colors[1:8:3])
sns.jointplot(x=df['registered'],y=df['count'],hue=df['day_type'])
plt.show()

"""*There's a linear correlation between Registered users and Count for weekdays.*

**Outlier Detection with BoxPlot**
"""

df_num=df.select_dtypes(include=np.number)
sns.set_palette(colors[1:8:3])
for col in enumerate(df_num):
  plt.figure(figsize=(5,3))
  sns.boxplot(x=col[1], data=df_num)
  plt.show()

"""From the above plots we can confirm that Windspeed, Casual, Registered, Count all these columns have outliers."""

def z_outlier(column):
  z_score=scipy.stats.zscore(df[column])
  n=df.index[np.abs(z_score)>3]
  outliers=pd.Series(n).count()
  percentage= outliers*100/df.shape[0]
  print(f"The number of outliers present in column '{column}' is {outliers} which is {round(percentage,2)} % of the total data")
  return

wind_olr=z_outlier('windspeed')

z_outlier('count')

z_score_threshold=3
z_score=np.abs(scipy.stats.zscore(df['count']))
iqr=df[z_score<z_score_threshold]

plt.rcParams["figure.figsize"] = [18,6]
sns.set_palette(colors[3:8:3])
plt.subplot(1,2,1)
sns.histplot(df['count'],kde=True)
plt.ylabel("Frequency")
plt.subplot(1,2,2)
sns.histplot(iqr['count'],kde=True)
plt.xlabel("Count with Outliers Removed")
plt.ylabel("Frequency")
plt.show()

n=20000
Count_CLT=[np.mean(df['count'].sample(30)) for i in range(n)]
plt.figure(figsize=(7,5))
sns.set_palette(colors[3:8:3])
sns.histplot(Count_CLT, kde=True)
plt.show()

"""Even though the original distribution of Count doesn't follow Gaussian distribution but it follows CLT, so we can apply parametric tests on it."""

df.groupby('season')['count'].sum()

plt.figure(figsize=(9,5))
sns.set_palette(colors[3:8:3])
sns.boxplot(x='season',y='count',data=df)
plt.show()

"""*In the above boxplot, we can see that the variances for Spring and Winter differ a lot from Summer and Fall.*"""

spring=df[df['season']=='Spring']['count']
summer=df[df['season']=='Summer']['count']
fall=df[df['season']=='Fall']['count']
winter=df[df['season']=='Winter']['count']

stats, p_value=stats.kruskal(spring,summer,fall,winter)
print(f"Kruskal-Wallis Statistic: {stats}")
print(f"P-value: {p_value}")

"""**The P-value for Kruskal Wallis Test came out to be pretty low, which means it the count depends on season.**"""

stat,p_value=stats.levene(summer,fall)
print(f"Levene's Statistic: {stat}")
print(f"P-value: {p_value}")

"""*As the P-value for Levene's test is quite high, we can conclude variance of Summer and Fall is not significantly different, so we will check whether the mean count for summer and fall is significantly different or not.*"""

t_stat,p_val=stats.ttest_ind(summer,fall)
print(f"F-statistic: {t_stat}")
print(f"P-value: {p_val}")

"""**The P-value for T test came out to be quite low, which means the mean of Count is quite different for Summer or Fall as well.**"""

plt.figure(figsize=(9,5))
sns.set_palette(colors[3:8:3])
sns.boxplot(x='day_type',y='count',data=df)
plt.show()

weekday=df[df['day_type']=='weekday']['count']
weekend=df[df['day_type']=='weekend']['count']
holiday=df[df['day_type']=='holiday']['count']

stat,p_value=stats.levene(weekday,weekend,holiday)
print(f"Levene's Statistic: {stat}")
print(f"P-value: {p_value}")

"""*The P-Value for Levene's test turned out to be very high, which means that all the three groups has almost equal variances. We can go for Anova Test to test whether the mean of those groups significantly differ or not.*"""

stat,p_value=stats.f_oneway(weekday,weekend,holiday)
print(f"F-statistic: {stat}")
print(f"P-value: {p_value}")

"""The P-Value for Anova Test is high, so we cannot reject Null Hypothesis.<br> **There is no significant difference between in bike demand across weekdays, weekends and holidays.**"""

plt.figure(figsize=(9,5))
sns.set_palette(colors[3:8:3])
sns.boxplot(x='weather',y='count',data=df)
plt.show()

"""*The distribution of Count for the fourth type of weather is very concentrated. Although the variances of all the groups seem to differ from the box plot, but still we will go for a statistical test for first and second type of weather.*"""

stats, p_value=stats.levene(df[df['weather']=='1']['count'],df[df['weather']=='2']['count'])
print(f"Levene's Statistic: {stats}")
print(f"P-value: {p_value}")

"""*The P value for the Levene's test turned out to be very low, which means there's no homogenity of variance across the first and second group of weather.*"""

stats, p_value=stats.kruskal(df[df['weather']=='1']['count'],df[df['weather']=='2']['count'],df[df['weather']=='3']['count'],df[df['weather']=='4']['count'])
print(f"Kruskal-Wallis Statistic: {stats}")
print(f"P-value: {p_value}")

"""**The P Value of Kruskal Wallis Test is too low, which means the mean Count of each group of Weather varies significantly.**"""

import scipy.stats as stats

# H0: Weather and Season are not dependent.
# Ha: Weather depends on Season.
chi2_stat, p_value, dof, expected=stats.chi2_contingency(pd.crosstab(df['weather'],df['season']))
print(f"Chi-Square Statistic: {chi2_stat}")
print(f"P-value: {p_value}")
print(f"Degrees of Freedom: {dof}")

"""**P value is much less, which means Weather is dependent on the Season.**"""

data=df.groupby(['weather','season'])['count'].mean().reset_index()

data['weather'].unique()

data['season'].unique()

data=df.groupby(['weather','season'])['count'].mean().reset_index()
data['weather_season']=data['season'].astype(str)+'_'+data['weather'].astype(str)
data.rename(columns={'count':'season_weather_score'}, inplace=True)
data.drop(['weather','season'], axis=1,inplace=True)
data.dropna(inplace=True)
data.head()

from scipy.stats import kruskal
st, p_value=kruskal(*[data[data['weather_season'] == cat]['season_weather_score'] for cat in data['weather_season'].unique()])
p_value

"""**The P value of Kruskal Wallis test is pretty high, indicating that there's no enough proof to believe that our new feature weather_season is a good predictor for count of rents.**"""

numerical_columns=df.select_dtypes(include=np.number)
numerical_columns.head(2)

from matplotlib.colors import LinearSegmentedColormap
custom_cmap = LinearSegmentedColormap.from_list("custom_cmap", colors)

sns.set(font_scale=1.0)
plt.figure(figsize=(10,8))
correlation_values = numerical_columns.corr(method = 'spearman')
sns.heatmap(correlation_values, linewidths=0.01, square=True, annot=True, cmap=custom_cmap, linecolor="black")
plt.title('Correlation between features')
plt.show()

"""*No factor seems to directly correlate with Count. Temperature seems to moderately affect the number of bike rents by casual users.If Temperature increases casual users are more likely to rent bikes but there are other factors too which affect this. So further investigation is needed.*"""

sizes=[df['registered'].sum(),df['casual'].sum()]
sizes

plt.figure(figsize=(4,4))
plt.pie(sizes, labels=['Registered','Casual'], autopct='%1.1f%%',startangle=90,colors=colors[3:8:3])
plt.title('Registered vs Casual Users')
plt.axis('equal')
plt.show()

_reg_olr=z_outlier('registered')

cas_olr=z_outlier('casual')

import scipy.stats as stats
for i in ['registered', 'casual']:
  iqr=df[~np.abs(stats.zscore(df[i])<3)]
iqr.head(2)

plt.rcParams["figure.figsize"] = [18,18]
sns.set_palette(colors[3:8:3])
fig, axs = plt.subplots(2, 2,figsize=(10,10))
sns.boxplot(x='day_type', y='registered', data=df, ax=axs[0, 0])
sns.boxplot(x='day_type', y='casual', data=df, ax=axs[0, 1])
sns.boxplot(x='day_type', y='registered', data=iqr, ax=axs[1, 0])
sns.boxplot(x='day_type', y='casual', data=iqr, ax=axs[1, 1])

axs[0, 0].set_title("Registered Users (Original Data)")
axs[0, 1].set_title("Casual Users (Original Data)")
axs[1, 0].set_title("Registered Users (Outliers Removed)")
axs[1, 1].set_title("Casual Users (Outliers Removed)")

plt.tight_layout()

plt.show()

"""*As we can see the variance looks significantly different for each type of day. So we'll go for Levene's test first.*"""

weekend_reg=df[df['day_type']=='weekend']['registered']
weekend_cas=df[df['day_type']=='weekend']['casual']
weekday_reg=df[df['day_type']=='weekday']['registered']
weekday_cas=df[df['day_type']=='weekday']['casual']
holiday_reg=df[df['day_type']=='holiday']['registered']
holiday_cas=df[df['day_type']=='holiday']['casual']

# H0: Both the group has similar variance
# Ha: The variance significantly differ among the groups
# Significance Level: alpha= 0.05

import scipy.stats as stats
stats,p_Value=stats.levene(weekend_reg,weekday_reg,holiday_reg)
print(f"Levene's Statistic: {stats}")
print(f"P-value: {p_Value}")

from scipy.stats import levene, kruskal, ttest_ind
stats,p_Value=levene(weekend_cas,weekday_cas,holiday_cas)
print(f"Levene's Statistic: {stats}")
print(f"P-value: {p_Value}")

"""*In both the cases, we reject null hypothesis.*"""

# H0: Both the group has similar mean
# Ha: The mean significantly differ among the groups
# Significance Level: alpha= 0.05

stats,p_Value=ttest_ind(weekend_reg,holiday_reg)
print(f"t Statistic: {stats}")
print(f"P-value: {p_Value}")

stats,p_Value=ttest_ind(weekday_reg,weekend_reg)
print(f"t Statistic: {stats}")
print(f"P-value: {p_Value}")

"""*The P_value for t test between weekend_reg and holiday_reg came out pretty high,whereas between weekday_reg and weekend_reg it came out too low.In first case, we can't reject null hypothesis which means count of bike rents is similar in two groups, whereas in second case we reject null hypothesis. That signifies count of bike rents in weekdays is quite different.*  """

categories = ['Weekend', 'Weekday', 'Holiday']
values = [weekend_reg.sum(), weekday_reg.sum(), holiday_reg.sum()]

plt.figure(figsize=(10,4))
sns.set_palette(colors[3:8:3])
sns.barplot(y=categories,x=values,width=0.4)
plt.ticklabel_format(style='plain',axis='x')
plt.xlabel("Count of Bike Rents")
plt.title("Bike Rentals by Day Type(Registered Users) ")
plt.show()

"""**Weekday rentals are dominant among registered bike users.**"""

categories = ['Weekend', 'Weekday', 'Holiday']
values = [weekend_cas.sum(), weekday_cas.sum(), holiday_cas.sum()]

plt.figure(figsize=(10,4))
sns.set_palette(colors[3:8:3])
sns.barplot(y=categories,x=values,width=0.4)
plt.xlabel("Count of Bike Rents")
plt.title("Bike Rentals by Day Type(Casual Users) ")
plt.show()

# significance level: alpha= 0.05
stats,p_Value=ttest_ind(weekend_cas,holiday_cas)
print(f"t Statistic: {stats}")
print(f"P-value: {p_Value}")

stats,p_Value=ttest_ind(weekday_cas,holiday_cas)
print(f"t Statistic: {stats}")
print(f"P-value: {p_Value}")

stats,p_Value=ttest_ind(weekend_cas,weekday_cas)
print(f"t Statistic: {stats}")
print(f"P-value: {p_Value}")

"""*We reject null hypothesis in all the cases, which mean of bike rentals are different in all the day type among casual users.*

# **Recommendation:**

*   Since the majority of users are registered, Yulu should prioritize catering to them. Most registered users rent bikes on weekdays, likely for commuting to work. Rental demand peaks at 8 AM and again between 5 and 6 PM, aligning with typical office hours. To enhance user experience, Yulu can ensure faster bike availability during these peak weekday hours and should introduce discounted bulk rentals for regular users.
*   Users are also likely to rent bikes on weekends as well, so special pricing on weekends.

*   January has the lowest rental activity. Yulu can introduce seasonal discount campaigns or referral bonuses to drive demand during this period
"""